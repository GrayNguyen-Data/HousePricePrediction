import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import random
import time
from database.load_data_layer_bronze import load_data_to_bronze_layer
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException


def crawler_and_load_data(max_pages=70):
    # S·ª≠ d·ª•ng try...finally ƒë·ªÉ ƒë·∫£m b·∫£o driver ƒë∆∞·ª£c ƒë√≥ng (kh·∫Øc ph·ª•c l·ªói WinError 6)
    driver = None 
    try:
        options = uc.ChromeOptions()
        # V·∫´n gi·ªØ t√πy ch·ªçn ch·ªëng bot c∆° b·∫£n
        options.add_argument("--disable-blink-features=AutomationControlled") 
        # T√πy ch·ªçn ƒë·ªÅ xu·∫•t: Ch·∫°y ·∫©n (headless) ƒë·ªÉ tƒÉng t·ªëc v√† gi·∫£m r·ªßi ro b·ªã block
        # options.add_argument('--headless') 

        driver = uc.Chrome(options=options)
        # ƒê·∫∑t Implicit Wait ƒë·ªÉ gi√∫p driver t√¨m ki·∫øm ph·∫ßn t·ª≠ nhanh h∆°n (t·ªëi ƒëa 10 gi√¢y)
        driver.implicitly_wait(10)
        
        base_url = 'https://batdongsan.com.vn/ban-nha-rieng-tp-hcm'
        
        for page in range(43, max_pages + 1):
            # T·ªëi ∆∞u logic t·∫°o URL: ƒê∆°n gi·∫£n v√† ch√≠nh x√°c
            url = f"{base_url}/p{page}" if page > 1 else base_url

            print(f"\nüåê ƒêang c√†o trang {page}: {url}")
            driver.get(url)
            
            # T·ªëi ∆∞u: Thay time.sleep d√†i b·∫±ng Explicit Wait
            try:
                # Ch·ªù ƒë·ª£i √≠t nh·∫•t m·ªôt link tin chi ti·∫øt xu·∫•t hi·ªán
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'js__product-link-for-product-id'))
                )
            except TimeoutException:
                print(f"‚ùå Timeout! Kh√¥ng t√¨m th·∫•y tin n√†o ·ªü trang {page}. C√≥ th·ªÉ ƒë√£ b·ªã block ho·∫∑c h·∫øt trang.")
                # N·∫øu h·∫øt trang/b·ªã block, n√™n tho√°t v√≤ng l·∫∑p ngo√†i
                break

            # Scroll human-like (tƒÉng t·ªëc ƒë·ªô, gi·∫£m s·ªë l·∫ßn scroll)
            for _ in range(5):
                driver.execute_script("window.scrollBy(0, 800);")
                time.sleep(random.uniform(0.3, 0.8))

            html = driver.page_source
            soup = BeautifulSoup(html, 'lxml')
            links = soup.find_all('a', class_='js__product-link-for-product-id')

            print(f"T√¨m th·∫•y {len(links)} tin ·ªü trang {page}")

            # C√†o t·ª´ng link chi ti·∫øt
            for infor in links:
                href = "https://batdongsan.com.vn" + infor.get('href', '')
                try:
                    driver.get(href)
                    
                    # T·ªëi ∆∞u: Explicit Wait cho ti√™u ƒë·ªÅ tin chi ti·∫øt
                    WebDriverWait(driver, 30).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 're__pr-title'))
                    )
                    
                    # Scroll nh·∫π ƒë·ªÉ t·∫£i n·ªôi dung ƒë·ªông (gi·∫£m th·ªùi gian ngh·ªâ)
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight/3);")
                    time.sleep(random.uniform(1, 1.5))

                    detail_html = driver.page_source
                    detail_soup = BeautifulSoup(detail_html, 'lxml')

                    # ----------------- LOGIC TR√çCH XU·∫§T D·ªÆ LI·ªÜU -----------------
                    # ... (Logic tr√≠ch xu·∫•t d·ªØ li·ªáu gi·ªØ nguy√™n, n√≥ ƒë√£ ƒë√∫ng) ...
                    
                    data = {
                        'title': None, 'address': None, 'area': None, 'floors': None,
                        'furniture': None, 'bedrooms': None, 'bathrooms': None,
                        'price': None, 'price_m2': None, 'posted_date': None, 'link': href
                    }
                    
                    # L·∫•y title
                    title_tag = detail_soup.find('h1', class_='re__pr-title pr-title js__pr-title')
                    if title_tag:
                        data['title'] = title_tag.text.strip()

                    # L·∫•y address
                    address_tag = detail_soup.find('span', class_="re__pr-short-description js__pr-address")
                    if address_tag:
                        data['address'] = address_tag.text.strip()
                        
                    # C√°c thu·ªôc t√≠nh kh√°c (Di·ªán t√≠ch, S·ªë t·∫ßng,...)
                    items = detail_soup.find_all('div', class_='re__pr-specs-content-item')
                    for item in items:
                        label_tag = item.find('span', class_='re__pr-specs-content-item-title')
                        value_tag = item.find('span', class_='re__pr-specs-content-item-value')
                        if not label_tag or not value_tag: continue
                        label = label_tag.text.strip()
                        value = value_tag.text.strip()
                        if label == "Di·ªán t√≠ch": data['area'] = value
                        elif label == "S·ªë t·∫ßng": data['floors'] = value
                        elif label == "N·ªôi th·∫•t": data['furniture'] = value
                        elif label == "S·ªë ph√≤ng ng·ªß": data['bedrooms'] = value
                        elif label == "S·ªë ph√≤ng t·∫Øm, v·ªá sinh": data['bathrooms'] = value

                    # Gi√°
                    prices = detail_soup.find_all('div', class_='re__pr-short-info-item js__pr-short-info-item')
                    for item in prices:
                        label_tag = item.find('span', class_='title')
                        value_tag = item.find('span', class_='value')
                        ext_tag = item.find('span', class_='ext')
                        if label_tag and label_tag.text.strip() == "Kho·∫£ng gi√°":
                            if value_tag: data['price'] = value_tag.text.strip()
                            if ext_tag: data['price_m2'] = ext_tag.text.strip()

                    # Ng√†y ƒëƒÉng
                    date = detail_soup.find_all('div', class_='re__pr-short-info-item js__pr-config-item')
                    for item in date:
                        label_tag = item.find('span', class_='title')
                        value_tag = item.find('span', class_='value')
                        if label_tag and label_tag.text.strip() == "Ng√†y ƒëƒÉng":
                            if value_tag: data['posted_date'] = value_tag.text.strip()

                    # 6. L∆ØU D·ªÆ LI·ªÜU
                    load_data_to_bronze_layer(data)
                    print(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu cho {data.get('title', href)}")

                except Exception as e:
                    print(f"‚ùå L·ªói khi x·ª≠ l√Ω {href}: {e}")
                
                # T·ªëi ∆∞u: Ngh·ªâ ng·∫Øn v√† ng·∫´u nhi√™n gi·ªØa c√°c tin chi ti·∫øt (gi·∫£m t·ª´ 15-18s xu·ªëng 3-5s)
                time.sleep(random.uniform(3, 5)) 
            
            # T·ªëi ∆∞u: Ngh·ªâ gi·ªØa c√°c trang (gi·∫£m t·ª´ 10-15s xu·ªëng 5-10s)
            time.sleep(random.uniform(5, 10))
            
    except WebDriverException as overall_e:
        # B·∫Øt c√°c l·ªói li√™n quan ƒë·∫øn driver (nh∆∞ SessionNotCreatedException, ConnectionRefused)
        print(f"‚ùå L·ªói nghi√™m tr·ªçng c·ªßa WebDriver (Ki·ªÉm tra phi√™n b·∫£n Chrome/Driver): {overall_e}")

    except Exception as overall_e:
        print(f"‚ùå L·ªói chung trong qu√° tr√¨nh c√†o d·ªØ li·ªáu: {overall_e}")

    finally:
        # 7. FIX WINERROR 6: ƒê√≥ng driver an to√†n
        if driver:
            try:
                driver.quit()
                print("ƒê√£ ƒë√≥ng Driver an to√†n.")
            except Exception as close_error:
                # Ngo·∫°i l·ªá b·ªã b·ªè qua WinError 6 th∆∞·ªùng ƒë∆∞·ª£c b·∫Øt ·ªü ƒë√¢y
                print(f"C·∫£nh b√°o: L·ªói khi ƒë√≥ng Driver (ƒë√£ ƒë∆∞·ª£c b·ªè qua): {close_error}")